name: Run Tests

on:
  push:
    branches:
      - main
    paths:
      - 'data-generation/**'
      - 'validation/**'
      - 'notebooks/**'
      - '*.py'
      - 'requirements*.txt'
  pull_request:
    paths:
      - 'data-generation/**'
      - 'validation/**'
      - 'notebooks/**'
      - '*.py'
      - 'requirements*.txt'
  workflow_dispatch:

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11']
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run Unit Tests
        run: |
          pytest validation/unit_tests/ \
            -v \
            --tb=short \
            --cov=data-generation \
            --cov-report=xml \
            --cov-report=html \
            --junitxml=test-results.xml

      - name: Upload Coverage
        uses: codecov/codecov-action@v4
        with:
          files: coverage.xml
          fail_ci_if_error: false

      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test-results.xml
            htmlcov/

  data-generator-tests:
    name: Data Generator Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Test Generators
        run: |
          # Test each generator can produce valid output
          python -c "
          from data_generation.generators.slot_machine_generator import SlotMachineGenerator
          from data_generation.generators.player_generator import PlayerGenerator
          from data_generation.generators.financial_generator import FinancialGenerator

          # Generate small samples to validate
          slot_gen = SlotMachineGenerator()
          slots = slot_gen.generate(100)
          assert len(slots) == 100, 'Slot generator failed'
          print('✅ Slot generator: OK')

          player_gen = PlayerGenerator()
          players = player_gen.generate(50)
          assert len(players) == 50, 'Player generator failed'
          print('✅ Player generator: OK')

          fin_gen = FinancialGenerator()
          txns = fin_gen.generate(100)
          assert len(txns) == 100, 'Financial generator failed'
          print('✅ Financial generator: OK')
          "

      - name: Validate Output Schemas
        run: |
          python -c "
          import json
          from pathlib import Path

          schema_dir = Path('data-generation/schemas')
          for schema_file in schema_dir.glob('*.json'):
              with open(schema_file) as f:
                  schema = json.load(f)
              print(f'✅ Schema valid: {schema_file.name}')
          "

  notebook-validation:
    name: Validate Notebooks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install nbformat nbconvert

      - name: Validate Notebook Syntax
        run: |
          python -c "
          import nbformat
          from pathlib import Path

          notebooks = list(Path('notebooks').rglob('*.ipynb'))
          errors = []

          for nb_path in notebooks:
              try:
                  with open(nb_path) as f:
                      nb = nbformat.read(f, as_version=4)
                  print(f'✅ Valid: {nb_path}')
              except Exception as e:
                  errors.append(f'❌ Invalid: {nb_path} - {e}')

          if errors:
              for e in errors:
                  print(e)
              exit(1)

          print(f'All {len(notebooks)} notebooks are valid')
          "

  great-expectations:
    name: Data Quality Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install great-expectations pandas

      - name: Run GE Checkpoints
        run: |
          cd validation/great_expectations

          # Run validation if sample data exists
          if [ -d "../../data-generation/output" ]; then
            great_expectations checkpoint run bronze_checkpoint || echo "No data to validate"
          else
            echo "No sample data found - skipping GE validation"
          fi

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, data-generator-tests, notebook-validation, great-expectations]
    if: always()
    steps:
      - name: Generate Summary
        run: |
          echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Data Generators | ${{ needs.data-generator-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Notebook Validation | ${{ needs.notebook-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Data Quality | ${{ needs.great-expectations.result == 'success' && '✅ Passed' || '⚠️ Skipped' }} |" >> $GITHUB_STEP_SUMMARY
