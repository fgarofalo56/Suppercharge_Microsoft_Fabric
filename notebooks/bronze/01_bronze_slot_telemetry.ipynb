{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bronze Layer: Slot Machine Telemetry Ingestion\n",
        "\n",
        "**Notebook:** `01_bronze_slot_telemetry`  \n",
        "**Layer:** Bronze (Raw)  \n",
        "**Purpose:** Ingest raw slot machine telemetry data into the Bronze Lakehouse\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates the Bronze layer ingestion pattern for slot machine telemetry data. The Bronze layer stores raw data with minimal transformation, preserving the original format for auditability.\n",
        "\n",
        "### Key Principles\n",
        "- **Append-only**: Never update or delete records\n",
        "- **Schema-on-read**: Accept data as-is\n",
        "- **Full history**: Maintain complete audit trail\n",
        "- **Metadata enrichment**: Add ingestion timestamp and source tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "# These can be overridden by pipeline parameters\n",
        "\n",
        "# Lakehouse configuration\n",
        "BRONZE_LAKEHOUSE = \"lh_bronze\"\n",
        "TABLE_NAME = \"bronze_slot_telemetry\"\n",
        "\n",
        "# Source configuration\n",
        "SOURCE_PATH = \"Files/raw/slot_telemetry/\"\n",
        "SOURCE_FORMAT = \"json\"  # Can be: json, csv, parquet\n",
        "\n",
        "# Processing configuration\n",
        "BATCH_SIZE = 100000\n",
        "ENABLE_SCHEMA_EVOLUTION = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, current_timestamp, input_file_name,\n",
        "    year, month, dayofmonth, hour,\n",
        "    sha2, concat_ws, to_timestamp\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType,\n",
        "    DoubleType, TimestampType, BooleanType\n",
        ")\n",
        "from delta.tables import DeltaTable\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Initialize Spark session (already available in Fabric)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Processing started: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Schema\n",
        "\n",
        "While Bronze typically uses schema-on-read, defining an expected schema helps with validation and documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected schema for slot telemetry data\n",
        "SLOT_TELEMETRY_SCHEMA = StructType([\n",
        "    StructField(\"event_id\", StringType(), False),\n",
        "    StructField(\"machine_id\", StringType(), False),\n",
        "    StructField(\"casino_id\", StringType(), True),\n",
        "    StructField(\"floor_location\", StringType(), True),\n",
        "    StructField(\"event_timestamp\", StringType(), False),\n",
        "    StructField(\"event_type\", StringType(), False),\n",
        "    StructField(\"denomination\", DoubleType(), True),\n",
        "    StructField(\"bet_amount\", DoubleType(), True),\n",
        "    StructField(\"win_amount\", DoubleType(), True),\n",
        "    StructField(\"jackpot_contribution\", DoubleType(), True),\n",
        "    StructField(\"credits_in\", IntegerType(), True),\n",
        "    StructField(\"credits_out\", IntegerType(), True),\n",
        "    StructField(\"credits_wagered\", IntegerType(), True),\n",
        "    StructField(\"credits_won\", IntegerType(), True),\n",
        "    StructField(\"games_played\", IntegerType(), True),\n",
        "    StructField(\"player_id\", StringType(), True),\n",
        "    StructField(\"session_id\", StringType(), True),\n",
        "    StructField(\"is_bonus_round\", BooleanType(), True),\n",
        "    StructField(\"rng_seed\", StringType(), True),\n",
        "    StructField(\"game_outcome\", StringType(), True),\n",
        "    StructField(\"paytable_version\", StringType(), True),\n",
        "    StructField(\"firmware_version\", StringType(), True),\n",
        "    StructField(\"error_code\", StringType(), True),\n",
        "    StructField(\"door_status\", StringType(), True),\n",
        "    StructField(\"meter_readings\", StringType(), True)  # JSON string for nested data\n",
        "])\n",
        "\n",
        "print(\"Schema defined with\", len(SLOT_TELEMETRY_SCHEMA.fields), \"fields\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Source Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_source_data(source_path: str, source_format: str, schema=None):\n",
        "    \"\"\"\n",
        "    Read data from source location with format detection.\n",
        "    \n",
        "    Args:\n",
        "        source_path: Path to source files\n",
        "        source_format: File format (json, csv, parquet)\n",
        "        schema: Optional schema to apply\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with source data\n",
        "    \"\"\"\n",
        "    reader = spark.read\n",
        "    \n",
        "    if schema:\n",
        "        reader = reader.schema(schema)\n",
        "    \n",
        "    if source_format == \"json\":\n",
        "        df = reader.option(\"multiLine\", True).json(source_path)\n",
        "    elif source_format == \"csv\":\n",
        "        df = reader.option(\"header\", True).option(\"inferSchema\", True).csv(source_path)\n",
        "    elif source_format == \"parquet\":\n",
        "        df = reader.parquet(source_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported format: {source_format}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Read the source data\n",
        "try:\n",
        "    df_raw = read_source_data(\n",
        "        source_path=f\"abfss://{BRONZE_LAKEHOUSE}@onelake.dfs.fabric.microsoft.com/{SOURCE_PATH}\",\n",
        "        source_format=SOURCE_FORMAT,\n",
        "        schema=SLOT_TELEMETRY_SCHEMA\n",
        "    )\n",
        "    print(f\"Read {df_raw.count()} records from source\")\n",
        "    df_raw.printSchema()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading source data: {e}\")\n",
        "    # For demo purposes, generate sample data\n",
        "    print(\"Generating sample data for demonstration...\")\n",
        "    df_raw = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sample Data (Demo Only)\n",
        "\n",
        "If source data is not available, generate sample data for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "\n",
        "def generate_sample_slot_data(num_records: int = 10000):\n",
        "    \"\"\"\n",
        "    Generate sample slot telemetry data for demonstration.\n",
        "    \"\"\"\n",
        "    event_types = [\"SPIN\", \"WIN\", \"JACKPOT\", \"BONUS_START\", \"BONUS_END\", \n",
        "                   \"CASH_IN\", \"CASH_OUT\", \"CARD_IN\", \"CARD_OUT\", \"DOOR_OPEN\", \"DOOR_CLOSE\"]\n",
        "    denominations = [0.01, 0.05, 0.25, 1.00, 5.00]\n",
        "    casino_ids = [\"CAS001\", \"CAS002\", \"CAS003\"]\n",
        "    floor_locations = [\"A1\", \"A2\", \"A3\", \"B1\", \"B2\", \"B3\", \"C1\", \"C2\", \"VIP1\", \"VIP2\"]\n",
        "    \n",
        "    data = []\n",
        "    base_time = datetime.now() - timedelta(days=7)\n",
        "    \n",
        "    for i in range(num_records):\n",
        "        event_type = random.choice(event_types)\n",
        "        denomination = random.choice(denominations)\n",
        "        \n",
        "        # Calculate realistic values based on event type\n",
        "        if event_type == \"SPIN\":\n",
        "            bet_amount = denomination * random.randint(1, 5) * random.randint(1, 20)\n",
        "            win_amount = bet_amount * random.choice([0, 0, 0, 0, 0.5, 1, 2, 5, 10, 50]) if random.random() > 0.6 else 0\n",
        "        elif event_type == \"JACKPOT\":\n",
        "            bet_amount = denomination * random.randint(1, 5) * random.randint(1, 20)\n",
        "            win_amount = random.uniform(1000, 50000)\n",
        "        else:\n",
        "            bet_amount = 0\n",
        "            win_amount = 0\n",
        "        \n",
        "        record = {\n",
        "            \"event_id\": str(uuid.uuid4()),\n",
        "            \"machine_id\": f\"SLT{random.randint(1000, 9999)}\",\n",
        "            \"casino_id\": random.choice(casino_ids),\n",
        "            \"floor_location\": random.choice(floor_locations),\n",
        "            \"event_timestamp\": (base_time + timedelta(seconds=random.randint(0, 604800))).isoformat(),\n",
        "            \"event_type\": event_type,\n",
        "            \"denomination\": denomination,\n",
        "            \"bet_amount\": round(bet_amount, 2),\n",
        "            \"win_amount\": round(win_amount, 2),\n",
        "            \"jackpot_contribution\": round(bet_amount * 0.01, 2) if event_type == \"SPIN\" else 0,\n",
        "            \"credits_in\": random.randint(0, 10000) if event_type == \"CASH_IN\" else 0,\n",
        "            \"credits_out\": random.randint(0, 10000) if event_type == \"CASH_OUT\" else 0,\n",
        "            \"credits_wagered\": int(bet_amount / denomination) if denomination > 0 else 0,\n",
        "            \"credits_won\": int(win_amount / denomination) if denomination > 0 else 0,\n",
        "            \"games_played\": random.randint(1, 500),\n",
        "            \"player_id\": f\"PLY{random.randint(10000000, 99999999)}\" if random.random() > 0.3 else None,\n",
        "            \"session_id\": str(uuid.uuid4()),\n",
        "            \"is_bonus_round\": event_type in [\"BONUS_START\", \"BONUS_END\"],\n",
        "            \"rng_seed\": str(uuid.uuid4())[:8],\n",
        "            \"game_outcome\": \"WIN\" if win_amount > 0 else \"LOSS\" if event_type == \"SPIN\" else None,\n",
        "            \"paytable_version\": f\"PT{random.randint(1, 5)}.{random.randint(0, 9)}\",\n",
        "            \"firmware_version\": f\"FW{random.randint(1, 3)}.{random.randint(0, 9)}.{random.randint(0, 99)}\",\n",
        "            \"error_code\": f\"E{random.randint(100, 999)}\" if random.random() < 0.01 else None,\n",
        "            \"door_status\": \"OPEN\" if event_type in [\"DOOR_OPEN\"] else \"CLOSED\",\n",
        "            \"meter_readings\": json.dumps({\n",
        "                \"coin_in\": random.randint(100000, 9999999),\n",
        "                \"coin_out\": random.randint(80000, 8999999),\n",
        "                \"jackpot\": random.randint(0, 100000),\n",
        "                \"games\": random.randint(10000, 999999)\n",
        "            })\n",
        "        }\n",
        "        data.append(record)\n",
        "    \n",
        "    return spark.createDataFrame(data, schema=SLOT_TELEMETRY_SCHEMA)\n",
        "\n",
        "# Generate sample data if source data not available\n",
        "if df_raw is None:\n",
        "    df_raw = generate_sample_slot_data(50000)\n",
        "    print(f\"Generated {df_raw.count()} sample records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Bronze Metadata\n",
        "\n",
        "Enrich raw data with ingestion metadata for lineage and auditing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_bronze_metadata(df):\n",
        "    \"\"\"\n",
        "    Add standard Bronze layer metadata columns.\n",
        "    \n",
        "    Columns added:\n",
        "    - _ingestion_timestamp: When the record was ingested\n",
        "    - _source_file: Source file name (if available)\n",
        "    - _batch_id: Unique identifier for this batch\n",
        "    - _record_hash: SHA-256 hash of key fields for deduplication\n",
        "    - _year, _month, _day: Partition columns based on event timestamp\n",
        "    \"\"\"\n",
        "    batch_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    \n",
        "    df_enriched = df \\\n",
        "        .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
        "        .withColumn(\"_source_file\", lit(SOURCE_PATH)) \\\n",
        "        .withColumn(\"_batch_id\", lit(batch_id)) \\\n",
        "        .withColumn(\"_record_hash\", sha2(concat_ws(\"|\", \n",
        "            col(\"event_id\"), \n",
        "            col(\"machine_id\"), \n",
        "            col(\"event_timestamp\")\n",
        "        ), 256)) \\\n",
        "        .withColumn(\"_event_ts\", to_timestamp(col(\"event_timestamp\"))) \\\n",
        "        .withColumn(\"_year\", year(col(\"_event_ts\"))) \\\n",
        "        .withColumn(\"_month\", month(col(\"_event_ts\"))) \\\n",
        "        .withColumn(\"_day\", dayofmonth(col(\"_event_ts\")))\n",
        "    \n",
        "    return df_enriched\n",
        "\n",
        "# Add metadata\n",
        "df_bronze = add_bronze_metadata(df_raw)\n",
        "print(\"Metadata columns added\")\n",
        "df_bronze.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Quality Checks (Basic)\n",
        "\n",
        "Perform minimal quality checks at Bronze layer - mainly ensuring data was read correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bronze_quality_checks(df):\n",
        "    \"\"\"\n",
        "    Perform basic quality checks for Bronze layer.\n",
        "    \n",
        "    Returns:\n",
        "        dict with quality metrics\n",
        "    \"\"\"\n",
        "    total_records = df.count()\n",
        "    \n",
        "    # Check for required fields\n",
        "    null_event_id = df.filter(col(\"event_id\").isNull()).count()\n",
        "    null_machine_id = df.filter(col(\"machine_id\").isNull()).count()\n",
        "    null_timestamp = df.filter(col(\"event_timestamp\").isNull()).count()\n",
        "    \n",
        "    # Check for duplicates based on hash\n",
        "    distinct_records = df.select(\"_record_hash\").distinct().count()\n",
        "    duplicate_count = total_records - distinct_records\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"total_records\": total_records,\n",
        "        \"null_event_id\": null_event_id,\n",
        "        \"null_machine_id\": null_machine_id,\n",
        "        \"null_timestamp\": null_timestamp,\n",
        "        \"duplicate_records\": duplicate_count,\n",
        "        \"distinct_records\": distinct_records,\n",
        "        \"quality_score\": round((1 - (null_event_id + null_machine_id + null_timestamp) / total_records) * 100, 2)\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Run quality checks\n",
        "quality_metrics = bronze_quality_checks(df_bronze)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BRONZE QUALITY REPORT\")\n",
        "print(\"=\"*50)\n",
        "for key, value in quality_metrics.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write to Bronze Lakehouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_to_bronze(df, table_name: str, partition_cols: list = None):\n",
        "    \"\"\"\n",
        "    Write DataFrame to Bronze Lakehouse as Delta table.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to write\n",
        "        table_name: Target table name\n",
        "        partition_cols: Optional list of partition columns\n",
        "    \"\"\"\n",
        "    if partition_cols is None:\n",
        "        partition_cols = [\"_year\", \"_month\", \"_day\"]\n",
        "    \n",
        "    # Write with append mode (Bronze is append-only)\n",
        "    writer = df.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .option(\"mergeSchema\", str(ENABLE_SCHEMA_EVOLUTION).lower())\n",
        "    \n",
        "    if partition_cols:\n",
        "        writer = writer.partitionBy(*partition_cols)\n",
        "    \n",
        "    writer.saveAsTable(table_name)\n",
        "    \n",
        "    print(f\"Successfully wrote {df.count()} records to {table_name}\")\n",
        "\n",
        "# Write to Bronze\n",
        "write_to_bronze(df_bronze, TABLE_NAME, [\"_year\", \"_month\", \"_day\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the write by reading back\n",
        "df_verify = spark.table(TABLE_NAME)\n",
        "\n",
        "print(f\"\\nTable: {TABLE_NAME}\")\n",
        "print(f\"Total records: {df_verify.count()}\")\n",
        "print(f\"\\nPartitions:\")\n",
        "df_verify.select(\"_year\", \"_month\", \"_day\").distinct().show()\n",
        "\n",
        "print(f\"\\nSample records:\")\n",
        "df_verify.select(\n",
        "    \"event_id\", \"machine_id\", \"event_type\", \n",
        "    \"bet_amount\", \"win_amount\", \"_ingestion_timestamp\"\n",
        ").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary statistics\n",
        "summary = {\n",
        "    \"notebook\": \"01_bronze_slot_telemetry\",\n",
        "    \"layer\": \"Bronze\",\n",
        "    \"table\": TABLE_NAME,\n",
        "    \"records_processed\": quality_metrics[\"total_records\"],\n",
        "    \"quality_score\": quality_metrics[\"quality_score\"],\n",
        "    \"processing_time\": datetime.now().isoformat(),\n",
        "    \"status\": \"SUCCESS\"\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PROCESSING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(json.dumps(summary, indent=2))\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "microsoft": {
      "language": "python",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
