# ğŸ“… Day 1: Medallion Foundation

> ğŸ  Home > ğŸ“† POC Agenda > ğŸ—ï¸ Day 1

---

**Date:** `[INSERT DATE]`
**Duration:** 8 hours (9:00 AM - 5:00 PM)
**Focus:** Environment setup and Bronze layer implementation
**Audience:** Data Architects (4 participants)

---

## ğŸ“Š Day 1 Progress Tracker

```
Day 1 Progress: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0% Complete
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸŒ… Morning Session 1:   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Environment Setup
ğŸŒ… Morning Session 2:   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Bronze Layer Part 1
â˜€ï¸ Afternoon Session 3: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Bronze Layer Part 2
â˜€ï¸ Afternoon Session 4: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Silver Layer Start
```

---

## ğŸ“‹ Materials Checklist

Before starting, ensure the following are ready:

### Environment
- [ ] Azure account with Fabric access confirmed
- [ ] F64 capacity available
- [ ] All participants have workspace access

### Data & Code
- [ ] Sample data generated and staged
- [ ] Notebooks ready for distribution
- [ ] Data generation scripts tested

### Logistics
- [ ] Projector/display working
- [ ] All participants present
- [ ] WiFi credentials shared

---

## ğŸ—“ï¸ Daily Schedule At-a-Glance

| Time | Duration | Session | Type |
|:----:|:--------:|---------|:----:|
| 9:00-10:30 | 1.5 hr | ğŸŒ… Environment Setup | ğŸ‘¥ Hands-on |
| 10:30-10:45 | 15 min | â˜• Break | - |
| 10:45-12:30 | 1.75 hr | ğŸŒ… Bronze Layer Part 1 | ğŸ‘¥ Hands-on |
| 12:30-13:30 | 1 hr | ğŸ½ï¸ Lunch | - |
| 13:30-15:00 | 1.5 hr | â˜€ï¸ Bronze Layer Part 2 | ğŸ‘¥ Hands-on |
| 15:00-15:15 | 15 min | â˜• Break | - |
| 15:15-16:45 | 1.5 hr | â˜€ï¸ Silver Layer Start | ğŸ‘¥ Hands-on |
| 16:45-17:00 | 15 min | ğŸ’¬ Day 1 Wrap-up | Discussion |

---

## ğŸŒ… Morning Session 1: Environment Setup (9:00 - 10:30)

### ğŸ¯ Session Objectives

| Objective | Duration | Status |
|-----------|:--------:|:------:|
| Create Fabric workspace with proper capacity settings | 30 min | â¬œ |
| Configure Lakehouse architecture for Bronze/Silver/Gold layers | 30 min | â¬œ |
| Generate and stage sample casino data | 30 min | â¬œ |

---

### ğŸ“ Activity 1.1: Workspace Configuration (30 min)

<table>
<tr>
<td width="60%">

**Steps:**

```
1. Navigate to app.fabric.microsoft.com
2. Create workspace: casino-fabric-poc
3. Configure capacity: F64
4. Enable workspace features:
   - Lakehouse
   - Data Engineering
   - Data Science
   - Real-Time Intelligence
```

</td>
<td width="40%">

**Validation Checkpoint:**

- [ ] Workspace created and accessible
- [ ] F64 capacity assigned
- [ ] All workload types available

</td>
</tr>
</table>

---

### ğŸ“ Activity 1.2: Lakehouse Creation (30 min)

Create three Lakehouses for medallion architecture:

| Lakehouse | Purpose | Schema | Status |
|-----------|---------|--------|:------:|
| `lh_bronze` | Raw data ingestion | Append-only, schema-on-read | â¬œ |
| `lh_silver` | Cleansed/validated | Schema enforced, partitioned | â¬œ |
| `lh_gold` | Business-ready | Star schema, optimized | â¬œ |

**Steps:**

1. In workspace, click **+ New** > **Lakehouse**
2. Create `lh_bronze` first
3. Repeat for `lh_silver` and `lh_gold`
4. Verify SQL endpoints are created

---

### ğŸ“ Activity 1.3: Generate Sample Data (30 min)

Use the data generators to create realistic casino data:

```bash
# Navigate to data-generation folder
cd data-generation

# Install dependencies
pip install -r requirements.txt

# Generate 30 days of slot machine data (500K records)
python generate.py slot_machine --records 500000 --days 30 --output ../sample-data/

# Generate player profiles (10K records)
python generate.py player --records 10000 --output ../sample-data/

# Generate compliance data with CTR/SAR patterns
python generate.py compliance --records 5000 --days 30 --output ../sample-data/
```

**Data Files Created:**

| File | Description | Status |
|------|-------------|:------:|
| `slot_telemetry_YYYYMMDD.parquet` | Slot machine events | â¬œ |
| `player_profiles.parquet` | Player demographics | â¬œ |
| `table_games_YYYYMMDD.parquet` | Table game data | â¬œ |
| `financial_transactions_YYYYMMDD.parquet` | Cage operations | â¬œ |
| `compliance_events.parquet` | Regulatory filings | â¬œ |

---

## ğŸŒ… Morning Session 2: Bronze Layer Part 1 (10:45 - 12:30)

### ğŸ¯ Session Objectives

| Objective | Duration | Status |
|-----------|:--------:|:------:|
| Understand Bronze layer principles | 15 min | â¬œ |
| Upload sample data to landing zone | 15 min | â¬œ |
| Implement slot telemetry ingestion | 45 min | â¬œ |
| Implement table games ingestion | 45 min | â¬œ |

---

### ğŸ“ Activity 2.1: Bronze Layer Principles (15 min)

> ğŸ’¡ **Key Concepts**

```
Bronze Layer Rules:
â”œâ”€â”€ Store data exactly as received (raw)
â”œâ”€â”€ Append-only (no updates/deletes)
â”œâ”€â”€ Preserve source metadata
â”œâ”€â”€ Add ingestion timestamp
â””â”€â”€ Minimal transformation (only structural)
```

**Table Naming Convention:**
- Pattern: `bronze_{source}_{entity}`
- Example: `bronze_sas_slot_telemetry`

---

### ğŸ“ Activity 2.2: Upload Sample Data (15 min)

1. Open `lh_bronze` Lakehouse
2. Click **Get data** > **Upload files**
3. Navigate to `sample-data/` folder
4. Upload all generated Parquet files to `Files/landing/`

**Target Folder Structure:**

```
lh_bronze/
â”œâ”€â”€ Files/
â”‚   â””â”€â”€ landing/
â”‚       â”œâ”€â”€ slot_telemetry/
â”‚       â”œâ”€â”€ player_profiles/
â”‚       â”œâ”€â”€ table_games/
â”‚       â”œâ”€â”€ financial/
â”‚       â””â”€â”€ compliance/
â””â”€â”€ Tables/
```

---

### ğŸ“ Activity 2.3: Bronze Notebook - Slot Telemetry (45 min)

Create notebook `01_bronze_slot_telemetry`:

```python
# Cell 1: Configuration
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime

# Batch tracking
batch_id = datetime.now().strftime("%Y%m%d_%H%M%S")
source_path = "Files/landing/slot_telemetry/"

# Cell 2: Read raw data
df_raw = spark.read.parquet(f"abfss://{source_path}")

print(f"Records loaded: {df_raw.count()}")
print(f"Columns: {df_raw.columns}")

# Cell 3: Add Bronze metadata
df_bronze = df_raw \
    .withColumn("_ingestion_timestamp", current_timestamp()) \
    .withColumn("_batch_id", lit(batch_id)) \
    .withColumn("_source_file", input_file_name())

# Cell 4: Write to Bronze table
df_bronze.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .saveAsTable("bronze_slot_telemetry")

# Cell 5: Validate
spark.sql("SELECT COUNT(*) as total FROM bronze_slot_telemetry").show()
```

---

### ğŸ“ Activity 2.4: Bronze Notebook - Table Games (45 min)

Create notebook `02_bronze_table_games`:

```python
# Similar pattern for table games data
# Focus on RFID tracking and dealer transactions

df_raw = spark.read.parquet("Files/landing/table_games/")

df_bronze = df_raw \
    .withColumn("_ingestion_timestamp", current_timestamp()) \
    .withColumn("_batch_id", lit(batch_id)) \
    .withColumn("_source_file", input_file_name())

df_bronze.write \
    .format("delta") \
    .mode("append") \
    .saveAsTable("bronze_table_games")
```

> ğŸ‘¥ **Hands-On Exercise (30 min):**
> Participants implement table games notebook independently, then review together.

---

## â˜€ï¸ Afternoon Session 3: Bronze Layer Part 2 (13:30 - 15:00)

### ğŸ¯ Session Objectives

| Objective | Duration | Status |
|-----------|:--------:|:------:|
| Implement player profile ingestion | 30 min | â¬œ |
| Implement financial transaction ingestion | 30 min | â¬œ |
| Implement compliance data ingestion | 30 min | â¬œ |

---

### ğŸ“ Activity 3.1: Player Profile Ingestion (30 min)

Create notebook `03_bronze_player_profile`:

```python
# Player data with PII considerations
df_players = spark.read.parquet("Files/landing/player_profiles/")

# Hash SSN immediately at Bronze layer (compliance)
df_bronze = df_players \
    .withColumn("ssn_hash", sha2(col("ssn"), 256)) \
    .drop("ssn") \
    .withColumn("_ingestion_timestamp", current_timestamp()) \
    .withColumn("_batch_id", lit(batch_id))

df_bronze.write \
    .format("delta") \
    .mode("overwrite")  # Full refresh for dimension
    .saveAsTable("bronze_player_profile")
```

> âš ï¸ **Important:** SSN is hashed at ingestion for compliance. Original values are never stored.

---

### ğŸ“ Activity 3.2: Financial Transaction Ingestion (30 min)

Create notebook `04_bronze_financial_txn`:

```python
# Cage operations and financial transactions
df_financial = spark.read.parquet("Files/landing/financial/")

# Add CTR flag for amounts >= $10,000
df_bronze = df_financial \
    .withColumn("ctr_required", col("amount") >= 10000) \
    .withColumn("_ingestion_timestamp", current_timestamp()) \
    .withColumn("_batch_id", lit(batch_id))

df_bronze.write \
    .format("delta") \
    .mode("append") \
    .partitionBy("transaction_date") \
    .saveAsTable("bronze_financial_txn")
```

> ğŸ’° **CTR Threshold:** $10,000 triggers Currency Transaction Report requirements

---

### ğŸ“ Activity 3.3: Compliance Data Ingestion (30 min)

Create notebook `05_bronze_compliance`:

```python
# CTR, SAR, W-2G filings
df_compliance = spark.read.parquet("Files/landing/compliance/")

df_bronze = df_compliance \
    .withColumn("_ingestion_timestamp", current_timestamp()) \
    .withColumn("_batch_id", lit(batch_id)) \
    .withColumn("_regulatory_category",
        when(col("filing_type") == "CTR", "FinCEN")
        .when(col("filing_type") == "SAR", "FinCEN")
        .when(col("filing_type") == "W2G", "IRS")
        .otherwise("Other"))

df_bronze.write \
    .format("delta") \
    .mode("append") \
    .saveAsTable("bronze_compliance_filings")
```

---

## â˜€ï¸ Afternoon Session 4: Silver Layer Introduction (15:15 - 17:00)

### ğŸ¯ Session Objectives

| Objective | Duration | Status |
|-----------|:--------:|:------:|
| Understand Silver layer principles | 20 min | â¬œ |
| Begin slot data cleansing | 50 min | â¬œ |
| Implement data quality checks | 30 min | â¬œ |

---

### ğŸ“ Activity 4.1: Silver Layer Principles (20 min)

> ğŸ’¡ **Key Concepts**

```
Silver Layer Rules:
â”œâ”€â”€ Schema enforcement (strongly typed)
â”œâ”€â”€ Data quality validation
â”œâ”€â”€ Deduplication
â”œâ”€â”€ Standardization (dates, codes, formats)
â”œâ”€â”€ Business rule validation
â””â”€â”€ Audit trail (track changes)
```

**Transformation Patterns:**

| Pattern | Description |
|---------|-------------|
| NULL handling | Replace with defaults or flag |
| Date standardization | Convert to consistent format |
| Code mapping | Map machine types, zones |
| Outlier flagging | Identify anomalous values |

---

### ğŸ“ Activity 4.2: Silver Notebook - Slot Cleansing (50 min)

Create notebook `01_silver_slot_cleansed`:

```python
# Read from Bronze
df_bronze = spark.table("lh_bronze.bronze_slot_telemetry")

# Define expected schema
schema_silver = StructType([
    StructField("machine_id", StringType(), False),
    StructField("event_type", StringType(), False),
    StructField("event_timestamp", TimestampType(), False),
    StructField("coin_in", DecimalType(18, 2), True),
    StructField("coin_out", DecimalType(18, 2), True),
    StructField("games_played", IntegerType(), True),
    StructField("jackpot_amount", DecimalType(18, 2), True),
    StructField("zone", StringType(), True),
    StructField("denomination", DecimalType(5, 2), True)
])

# Data quality rules
df_cleaned = df_bronze \
    .filter(col("machine_id").isNotNull()) \
    .filter(col("event_timestamp").isNotNull()) \
    .withColumn("coin_in",
        when(col("coin_in") < 0, 0).otherwise(col("coin_in"))) \
    .withColumn("coin_out",
        when(col("coin_out") < 0, 0).otherwise(col("coin_out"))) \
    .dropDuplicates(["machine_id", "event_timestamp", "event_type"])

# Add quality score
df_silver = df_cleaned \
    .withColumn("_dq_score",
        (when(col("coin_in").isNotNull(), 1).otherwise(0) +
         when(col("zone").isNotNull(), 1).otherwise(0) +
         when(col("denomination").isNotNull(), 1).otherwise(0)) / 3 * 100)

# Write to Silver
df_silver.write \
    .format("delta") \
    .mode("overwrite") \
    .partitionBy("event_date") \
    .saveAsTable("lh_silver.silver_slot_cleansed")
```

---

### ğŸ“ Activity 4.3: Data Quality Dashboard (30 min)

Create validation queries:

```sql
-- Quality metrics
SELECT
    COUNT(*) as total_records,
    COUNT(CASE WHEN _dq_score = 100 THEN 1 END) as perfect_quality,
    AVG(_dq_score) as avg_quality_score,
    COUNT(DISTINCT machine_id) as unique_machines
FROM lh_silver.silver_slot_cleansed;

-- Quality by zone
SELECT
    zone,
    COUNT(*) as records,
    AVG(_dq_score) as avg_quality
FROM lh_silver.silver_slot_cleansed
GROUP BY zone
ORDER BY avg_quality DESC;
```

---

## âœ… Day 1 Validation Checklist

### Environment

| Checkpoint | Criteria | Status |
|------------|----------|:------:|
| Workspace | `casino-fabric-poc` created | â¬œ |
| Capacity | F64 assigned | â¬œ |
| Lakehouses | Three created (bronze, silver, gold) | â¬œ |

### Bronze Layer

| Table | Target | Status |
|-------|--------|:------:|
| `bronze_slot_telemetry` | 500K+ records | â¬œ |
| `bronze_table_games` | Populated | â¬œ |
| `bronze_player_profile` | 10K records | â¬œ |
| `bronze_financial_txn` | Populated | â¬œ |
| `bronze_compliance_filings` | Populated | â¬œ |

### Silver Layer (Started)

| Table | Criteria | Status |
|-------|----------|:------:|
| `silver_slot_cleansed` | Cleansed data | â¬œ |
| Data quality scores | Calculated | â¬œ |
| Deduplication | Verified | â¬œ |

---

## ğŸ“š Homework / Preparation for Day 2

### Required Reading

1. **Review Silver Layer Tutorial** (Tutorial 02)

### Exploration Tasks

2. **Explore Data:**
   - Run ad-hoc queries on Bronze tables
   - Identify data quality issues
   - Note any questions about transformation logic

### Background Reading

3. **Read About:**
   - SCD Type 2 patterns for player data
   - Delta Lake MERGE operations
   - Data quality frameworks (Great Expectations)

---

## ğŸ“˜ Instructor Notes

### Common Issues & Solutions

<table>
<tr>
<th>Issue</th>
<th>Solution</th>
</tr>
<tr>
<td>

**Capacity Not Available**

</td>
<td>

- Verify F64 is assigned to workspace
- Check Azure portal for capacity status

</td>
</tr>
<tr>
<td>

**File Upload Failures**

</td>
<td>

- Check file sizes (< 5GB per file recommended)
- Verify Parquet format is valid

</td>
</tr>
<tr>
<td>

**Notebook Timeouts**

</td>
<td>

- Increase Spark session timeout
- Use smaller data samples for testing

</td>
</tr>
</table>

### Key Discussion Points

- Why Bronze layer should be immutable
- Trade-offs between schema-on-read vs schema-on-write
- PII handling strategies at ingestion
- CTR threshold implications ($10K)

---

## ğŸ”— Quick Links

| Resource | Link |
|----------|------|
| Tutorial 00 | [Environment Setup](../tutorials/00-environment-setup/README.md) |
| Tutorial 01 | [Bronze Layer](../tutorials/01-bronze-layer/README.md) |
| Tutorial 02 | [Silver Layer](../tutorials/02-silver-layer/README.md) |

---

<div align="center">

**Day 1 Complete!**

```
Day 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Complete
Overall POC: â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 33% Complete
```

---

[â¬…ï¸ POC Overview](./README.md) | [Day 2: Transformations â¡ï¸](./day2-transformations-realtime.md)

</div>
